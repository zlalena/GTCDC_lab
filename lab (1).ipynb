{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557b8fe",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaf77d",
   "metadata": {},
   "source": [
    "# Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a806ec0",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "<br>\n",
    "In this notebook we demonstrate how to reconstruct an outdoor scene. This includes the following sections:\n",
    "\n",
    "[Part 1: Reconstructing a Scene with Gaussian Splatting](#part-1-reconstructing-a-scene-with-gaussian-splatting)<br>\n",
    "    &emsp;&emsp;1. [Gather Images](#gather-images)<br>\n",
    "    &emsp;&emsp;2. [Run Structure from Motion (COLMAP)](#run-structure-from-motion-colmap)<br>\n",
    "    &emsp;&emsp;3. [fVDB Reality Capture](#fvdb-reality-capture)<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;a) [Viewing a Sfm Scene](#viewing-a-sfm-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;b) [Reconstructing a Scene with Gaussian Splatting](#reconstructing-a-scene-with-gaussian-splatting) <br>\n",
    "    &emsp;&emsp;4. [Editing Gaussian Splats](#editing-gaussian-splats)<br>\n",
    "    &emsp;&emsp;5. [Create Isaac Sim Ready Files](#create-isaac-sim-ready-files)<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;a) [Convert to a Mesh](#convert-to-a-mesh) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;b) [Cropping and Converting](#cropping-and-converting) <br>\n",
    "[Part 2: Creating an Isaac Sim Scene](#Part-2-creating-an-isaac-sim-scene)<br>\n",
    "    &emsp;&emsp;6. [Running Isaac Sim](#running-isaac-sim)<br>\n",
    "    &emsp;&emsp;7. [Import the Assets](#import-the-assets)<br>\n",
    "    &emsp;&emsp;8. [Scene Setup](#scene-setup)<br>\n",
    "    &emsp;&emsp;9. [Optional: Splat Color Editing](#optional-splat-color-editing)<br>\n",
    "    &emsp;&emsp;10. [Save Scene](#save-scene)<br>\n",
    "    &emsp;&emsp;11. [Isaac Lab and Robot Locomotion](#isaac-lab-and-robot-locomotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2db670",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this lab we will learn to how to reconstruct an outdoor scene to test robots using NVIDIA fVDB framework and NVIDIA Omniverse NuRec rendering in Isaac Sim. We will walk through core reconstruction and rendering technologies, with a step-by-step workflow for simulating an entire outdoor environment for testing any robot. We will learn how to position images using structure from motion, train a 3D Gaussian splatting scene, extract 3D mesh, and convert to USD for simulation in Isaac Sim. This lab is split into two parts. In [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting), we will learn how to gather good data, use a SfM tool, and train a Gaussian splatting scene. In [Part 2](#Part-2-creating-an-isaac-sim-scene), we will switch focus to robotic simulation. We will import the files from [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting) into Isaac Sim and use Isaac Lab to move a Spot robot around the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1771364",
   "metadata": {},
   "source": [
    "## Part 1: Reconstructing a Scene with Gaussian Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53641d",
   "metadata": {},
   "source": [
    "### Gather Images \n",
    "Your reconstruction can only be as good as the data you start with, because of this it's crucial to have good images. For outdoor scenes there are some best practices you can follow to get good results. \n",
    "* If you have one main object or area you're focusing on it is recommended to circle or orbit it with a camera equipped aerial vehicle. More details on object centric collection can be found in [nerf_dataset_tips.md]( https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md).\n",
    "* For larger outdoor scenes with no one specific focus, either overlapping circular orbits, or traditional oblique mapping flight lines produce good quality results. An example flight mode for a popular drone model can be found [here](https://enterprise-insights.dji.com/blog/smart-oblique-capture).\n",
    "\n",
    "In this lab, we will show an example of an orbit collection using a video captured during flight. Frames are extracted every second or so for training. [FFmpeg](https://www.ffmpeg.org/), an open-source software for audio and video file processing, can be used to save individual frames from a video. Below is an example of how you can use FFmpeg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2efacd",
   "metadata": {},
   "source": [
    "<table style=\"width:80%;\">\n",
    "  <tr>\n",
    "    <td style=\"width:50%; text-align:center;\">\n",
    "      <img src=\"images/safety_park_camera_poses.png\" alt=\"First Image\" style=\"width:100%;\">\n",
    "      <div>Camera positions for a scene with a single area of focus. Cameras point towards the center of the scene wile orbiting around it. This scene is Safety Park, we will be using this scene for the rest of the lab.</div>\n",
    "    </td>\n",
    "    <td style=\"width:50%; text-align:center;\">\n",
    "      <img src=\"images/civil_air_patrol_camera_poses.png\" alt=\"Second Image\" style=\"width:100%;\">\n",
    "      <div>Camera positions for a large scene with no specific single area of focus. In this case the camera makes many orbits, covering about 7 square kilometers.</div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49150106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 1 frame per second from the original video to the **images_raw** folder as images.\n",
    "!ffmpeg -i ../../Data/safety_park/safety_park.webm -vf fps=1 ../../Results/safety_park/images_raw/output_frame_%04d.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba65225",
   "metadata": {},
   "source": [
    "For this lab we have curated data for you to use. We will be reconstructing Safety Park, a small pseudo town used for first responder training. Let's view the original video and some of the images of this park that we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39124830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play video\n",
    "! vlc ../../Data/safety_park/safety_park.webm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View images\n",
    "from IPython.display import Image\n",
    "Image(filename=\"../../Data/safety_park/images_raw/000059.jpg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e29ec-6690-4ff1-928c-eee1146ad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"../../Data/safety_park/images_raw/000114.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6ff48",
   "metadata": {},
   "source": [
    "### Run Structure from Motion (COLMAP)\n",
    "Many radiance field rendering methods, including 3D Gaussian Splatting, require the camera positions and a sparse point cloud of the scene for initialization. We currently have a folder of raw images, but no corresponding camera location or pose information. We can use a structure from motion tool (SfM) to estimate where the camera was for each image and to create a sparse point cloud of the scene. A commonly used SfM tool is [COLMAP](https://colmap.github.io/install.html), which we will use in combination with GLOMAP.  [GLOMAP](https://github.com/colmap/glomap) replaces COLMAP's mapper step, focusing on global positioning rather than incremental, and can run 10 or even 100 times faster than COLMAP's. Below we have provided example commands. There's no need to run them here as we have provided the result from an existing SfM run. You can run these commands on your own data by downloading COLMAP and GLOMAP and changing the **/Data/Path** to a directory that contains the **images_raw** folder of your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example COLMAP & GLOMAP commands\n",
    "\n",
    "# Run the feature extract to identify key points \n",
    "!colmap feature_extractor \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --ImageReader.camera_model PINHOLE \\\n",
    "    --ImageReader.single_camera 1 \\\n",
    "    --SiftExtraction.use_gpu 1 \\\n",
    "    --SiftExtraction.max_image_size 7096 \\\n",
    "    --SiftExtraction.max_num_features 20000 \\\n",
    "    --SiftExtraction.num_threads 14\n",
    "\n",
    "# Mature the features across images\n",
    "!colmap exhaustive_matcher \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --SiftMatching.use_gpu 1 \\\n",
    "    --SiftMatching.max_num_matches 60000 \\\n",
    "    --SiftMatching.guided_matching=true\n",
    "\n",
    "# Create a sparse 3D point cloud of the scene using GLOMAP's global mapper\n",
    "!glomap mapper \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --output_path /Data/Path/sparse \\\n",
    "    --GlobalPositioning.use_gpu 1 \\\n",
    "    --BundleAdjustment.use_gpu 1\n",
    "\n",
    "# Align 3d model by applying transformations \n",
    "!colmap model_aligner \\\n",
    "    --input_path /Data/Path/sparse/0 \\\n",
    "    --output_path /Data/Path/sparse/aligned \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --ref_is_gps 1 \\\n",
    "    --alignment_type ECEF \\\n",
    "    --alignment_max_error 3.0\n",
    "\n",
    "# Undistort original input images so they are as if they were taken with a pinhole camera\n",
    "!colmap image_undistorter \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --input_path /Data/Path/sparse/0 \\\n",
    "    --output_path /Data/Path \\\n",
    "    --output_type=COLMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de621e",
   "metadata": {},
   "source": [
    "Let's take a closer look at the files from the provided SfM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e583f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of directory in tree like format\n",
    "!tree ../../Data/safety_park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc8b87",
   "metadata": {},
   "source": [
    "Let's delve deeper into these files and folders.\n",
    "\n",
    "**images_raw**: Contains the original images <br>\n",
    "**images**: Contains undistorted images <br>\n",
    "**sparse**: Contains the sparse 3D reconstruction of the scene in folder 0. If COLMAP cannot register the images into 1 single scene, it will be split in to additional numbered folders. <br>\n",
    "**cameras.bin**: Camera intrinsics including camera IDs, camera models, and sensor dimensions. <br>\n",
    "**images.bin**: Camera poses and keypoints for all reconstructed images. <br>\n",
    "**points3D.bin**: The sparse 3D point cloud <br>\n",
    "To learn more these files COLMAP produces see the [COLMAP's Output Format page](https://colmap.github.io/format.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60144cc7",
   "metadata": {},
   "source": [
    "### fVDB Reality Capture\n",
    "Now that we have a SfM run we can visualize it and use it to train a Gaussian splatting scene. We will use an [fVDB](https://github.com/openvdb/fvdb-core) example project called [fVDB Reality Capture](https://github.com/openvdb/fvdb-reality-capture) to visualize and train. fVDB is a framework for encoding and operating on sparse voxel hierarchies of features in PyTorch. Voxels are like pixels but they are cubes instead of squares, making them three dimensional. Sparse means we only have voxels in areas of our scene that are occupied, voxels that don't contain anything are not represented. fVDB Reality Capture (fRC) is toolbox for reality capture tasks built on top of fVDB. It gathers the tools required to create an Isaac Sim compatible 3D reconstruction from a set of images and uses fVDB to make the tools fast and efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f879edf",
   "metadata": {},
   "source": [
    "#### Displaying a Sfm Scene\n",
    "\n",
    "We can use fRC to view and manipulate our SfM scene since it supports loading in capture data stored in different formats into a common representation that can be easily manipulated by users. To do this, data from a capture is stored in an **fvdb_reality_capture.SfmScene** object which acts as an in-memory representation of a 3D capture. We will use this object to manipulate and visualize our SfM scene. Let's import the required libraries and load our scene into a **SfmScene**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use a few libraries in this tutorial for logging, plotting, image processing, and\n",
    "# numerical computation\n",
    "import fvdb_reality_capture as frc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "# fvdb_reality_capture supports logging if enabled. We'll set the log level to logging.INFO which is somewhat\n",
    "# verbose but is informative for seeing\n",
    "# what's happening under the hood. If you want fewer logs, set level=logging.WARN. If you want\n",
    "# more logs, set level=logging.DEBUG\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load Safety Park SfM Scene\n",
    "sfm_scene: frc.SfmScene = frc.SfmScene.from_colmap(\"../../Data/safety_park\")\n",
    "print(\"Loaded SfM Scene\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c17268",
   "metadata": {},
   "source": [
    "Let's take a closer look at what a SfmScene scene consists of.\n",
    "* **SfmScene.cameras**: A dictionary mapping unique camera IDs to `SfmCameraMetadata` objects which describe camera parameters (e.g. projection matrices, distortion parameters). The size of this dictionary matches the number of cameras used to capture the scene (so if you scanned a scene with a pair of stereo cameras, then len(SfmScene.cameras) will be 2).\n",
    "* **SfmScene.images**: A list of SfmImageMetadata objects which contain paths to the images and optional masks, a reference to the camera (SfmCameraMetadata) used to capture each image, their camera-to-world (and inverse) transformations, and the set of 3D points visible in each image.\n",
    "* **points/points_rgb/points_err**: Numpy arrays of shape (N,3)/(N,3)/(N,) encoding known surface points in the scene, their RGB colors, and an unnormalized confidence value of the accuracy of that point. Note, N denotes the number of points here. <br>\n",
    "Now that we have a loaded SfmScene, let's plot some of its images, and the projected 3D points within those images.\n",
    "\n",
    "We can use this object to display our training images and project the 3D points from the sparse point cloud on to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ac46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize an image in an SfmScene and the 3D points visible from that images\n",
    "# projected onto the image plane as blue dots.\n",
    "def plot_image_from_scene(scene: frc.SfmScene, image_id: int, point_color: str):\n",
    "    # Here we get metadata about the image_id^th image in the dataset\n",
    "    image_meta: frc.SfmImageMetadata = scene.images[image_id]\n",
    "\n",
    "    # Here we get metatada about the camera that capured the image\n",
    "    camera_meta: frc.SfmCameraMetadata = image_meta.camera_metadata\n",
    "\n",
    "    # Get the visible 3d points for this image\n",
    "    # scene.points returns the set of all points in the scene, and\n",
    "    # image_meta.point_indices returns the indices of the points visible in the image\n",
    "    visible_points_3d: np.ndarray = scene.points[image_meta.point_indices]\n",
    "\n",
    "    # Project those points onto the image plane\n",
    "    # 1. Get the world -> camera space transform and projection matrix\n",
    "    #  - The world-to-camera matrix is a property of the image since it varies over images.\n",
    "    #  - The projection matrix is a property of the camera since it varies\n",
    "    #    per-camera but is the same for all images captured with that camera\n",
    "    # The world_to_camera_matrix is a (4, 4)-shaped numpy array encoding an SE(3) transform as a 4x4\n",
    "    # matrix.\n",
    "    # The projection matrix is a (3, 3)-shaped numpy array encoding a perspective projection\n",
    "    # from 3D to 2D.\n",
    "    world_to_cam_matrix: np.ndarray = image_meta.world_to_camera_matrix\n",
    "    projection_matrix: np.ndarray = camera_meta.projection_matrix\n",
    "\n",
    "    # 2. Transform world points to camera space using the world-to-camera matrix.\n",
    "    # The camera coordinate space is one where the camera center lies at the origin (0, 0, 0),\n",
    "    # the +Z axis is looking down the center of the image, the +X axis points along the right of\n",
    "    # the image, and the +Y axis points upward in the image.\n",
    "    visible_points_3d_cam_space = world_to_cam_matrix[:3,:3] @ visible_points_3d.T + world_to_cam_matrix[:3,3:4]\n",
    "\n",
    "    # 3. Transform camera space coordinates to image space (pixel) coordinates using the\n",
    "    #    projection matrix.\n",
    "    # The projection matrix transforms camera coordinates to image space and has the form\n",
    "    # [[fx, 0,  cx],\n",
    "    #  [0,  fy, cy],\n",
    "    #  [0,  0,  1]]\n",
    "    # where (fx, fy) are the x and y focal lengths (in pixel units), and (cx, cy) is the optical\n",
    "    # center (in pixel units).\n",
    "    visible_points_2d = projection_matrix @ visible_points_3d_cam_space\n",
    "    visible_points_2d /= visible_points_2d[2]\n",
    "\n",
    "    # Load the image and convert to RGB (OpenCV uses BGR by default)\n",
    "    loaded_image = cv2.imread(image_meta.image_path)\n",
    "    assert loaded_image is not None, f\"Failed to load image at {image_meta.image_path}\"\n",
    "    loaded_image = cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # If there's a mask associated with this image, use it to zero out\n",
    "    # masked pixels\n",
    "    if image_meta.mask_path:\n",
    "        mask = cv2.imread(image_meta.mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        assert mask is not None, f\"Failed to load mask at {image_meta.mask_path}\"\n",
    "        loaded_image *= (mask[..., np.newaxis] > 127)\n",
    "\n",
    "    # Plot the image and projected points as blue dots\n",
    "    plt.title(f\"SfmScene Image {image_id}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(loaded_image)\n",
    "    plt.scatter(visible_points_2d[0], visible_points_2d[1], color=point_color, marker=\".\", s=2)\n",
    "\n",
    "\n",
    "def plot_three_images(scene: frc.SfmScene, title: str, title_color=None, point_color=\"#5b5be2ff\"):\n",
    "    # Plot three images and points alongside each other\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.suptitle(title, color=title_color, fontweight='bold')\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plot_image_from_scene(scene, 8, point_color=point_color)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plot_image_from_scene(scene, 16, point_color=point_color)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plot_image_from_scene(scene, 32, point_color=point_color)\n",
    "    plt.show()\n",
    "\n",
    "plot_three_images(sfm_scene, \"Three images from the SfmScene and the projection of their visible points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e7bc8",
   "metadata": {},
   "source": [
    "If you want to know more about manipulation of SfmScenes in fvdb see the [Loading and Manipulating Sensor Data\n",
    "](https://github.com/openvdb/fvdb-reality-capture/blob/main/notebooks/sensor_data_loading_and_manipulation.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6b6d9",
   "metadata": {},
   "source": [
    "#### Train a Gaussian Splatting Scene \n",
    "In this section we will walk through training a Gaussian splatting scene from a set of images and a SfM output, but let's start with what a Gaussian splatting scene is.\n",
    "A Gaussian splatting scene is a trained 3D representation of a scene. The scene is made up of thousands to millions of 3-dimensional Gaussians called Gaussian splats or just splats. Each splat has an associated location, color, and opacity. Think of them as similar to voxels, or points in a point cloud. We start with splats in the same positions as the points in the sparse point cloud from our SfM run. During training splats are added, removed, split, and changed until the Gaussian splatting scene is capable of producing renders that are nearly identical to the training images. \n",
    "\n",
    "Now that we have an understanding of what a Gaussian splatting scene is, let's train our own. The fRC module provides a set of easy-to-use tools for optimizing a Gaussian splatting scene from data. The simplest way to get started is by creating a **SceneOptimizer** and calling **train**. This will produce a trained model which can be evaluated, saved, or used for downstream tasks (like meshing). This functionally is also written in **train.py**. We will show how to use both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99935e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvdb_reality_capture.training import Config, SceneOptimizationRunner\n",
    "# Create config so we can edit various parameters\n",
    "cfg = Config();\n",
    "cfg.eval_at_percent = [100]; # save evaluation for the end of training\n",
    "cfg.save_at_percent = [100]; # Don't save the model until the end of training\n",
    "cfg.refine_every_epoch = 4.5; # How often to refine Gaussians during optimization\n",
    "cfg.pose_opt_start_epoch = 100; # Epoch at which we start optimizing camera positions \n",
    "\n",
    "# Create a SceneOptimizationRunner\n",
    "scene_optimizer = SceneOptimizationRunner.new_run(\n",
    "        config=cfg, # add the config with the parameters we changed\n",
    "        dataset_path=\"../../Data/safety_park\", # Path to data\n",
    "        results_path=\"../../Results\", # Where to save results\n",
    "        image_downsample_factor=2, # Factor to scale images down by\n",
    "        disable_viewer=True, # Turn off live viewer\n",
    "        normalization_type=\"ecef2enu\", # Normalization type, using Earth-centered Earth-fixed coordinates to local east-north-up\n",
    "        points_percentile_filter= 2, # Amount of points that is too few for an image to contain and still use for training\n",
    "        use_every_n_as_val= -1, # Don't leave any images for validation at the end, use all for training\n",
    "    );\n",
    "\n",
    "# Train \n",
    "scene_optimizer.train();\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20085bc",
   "metadata": {},
   "source": [
    "Higher resolution images and a larger quantity of images will cause training to take longer. We have a pretrained Gaussian splatting scene for you to use. To stop the training, click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19024a12",
   "metadata": {},
   "source": [
    "Our **SceneOptimizer** was created with several different parameters. They effect when we save the model, how often the splats are refined, and more. There are many more parameters you can vary for training, run the training script with the help flag to see them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b0baa",
   "metadata": {},
   "source": [
    "Try changing and adding parameters to see how they effect training. To stop the run early you can click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing and adding parameters\n",
    "!python train.py --dataset-path ../../Data/safety_park --results_path ../../Results --image-downsample-factor 4 --cfg.max-epochs 100 --cfg.eval-at-percent 100 --disable_viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e441c7d",
   "metadata": {},
   "source": [
    "### Editing Gaussian Splats\n",
    "[Supersplat](https://github.com/playcanvas/supersplat) is an easy to use web browser-based tool for viewing and editing Gaussian splatting scenes. We will use a local version, but in the future you can access it from [superspl.at/editor](https://superspl.at/editor) or locally. Let's launch this tool and start editing our splats. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff111392",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../supersplat && npm run develop;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f2b2e",
   "metadata": {},
   "source": [
    "1. To see the viewer, go to [localhost:3000](http://localhost:3000/) in a web browser. \n",
    "2. In the upper left of the window navigate to to **File > Import**\n",
    "3. In the pop up file explore navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Data/safety_park_output**, and import the **safety_park_splats.ply**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1ea3e-47c3-49d0-a6df-10a390b03472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Supersplat demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/Supersplat_demo.webm\", width=800, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d3489",
   "metadata": {},
   "source": [
    "Now that the Gaussian splatting scene you created is displayed in Supersplat, you can move around, zoom in and out, and edit the splat. Experiment with the tools, such as the transform and selection tools. When your're done, shut down Supersplat by using the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41a03f",
   "metadata": {},
   "source": [
    "### Create Isaac Sim Ready Files\n",
    "The PLY file containing a Gaussian splat scene we get at the end of training is not compatible with Isaac. Let's go over how we can get our scene ready for use in Isaac Sim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37951e3",
   "metadata": {},
   "source": [
    "#### Convert to a Mesh\n",
    "In order for a robot to move around in Isaac Sim there needs to be a surface it can interact with. Gaussian splatting scenes cannot represent a solid surface in Isaac Sim, so we need to convert our splat to something capable of acting as a surface a robot can collide with. Specifically we will convert the splat into a mesh, using TSDF (Truncated Signed Distance Field) fusion from depth maps predicted from the Gaussian splat model and the DLNR foundation model for predicting stereo depth.\n",
    "The TSDF fusion algorithm is based on the paper: [KinectFusion: Real-Time Dense Surface Mapping and Tracking](https://www.microsoft.com/en-us/research/publication/kinectfusion-real-time-3d-reconstruction-and-interaction-using-a-moving-depth-camera/).\n",
    "The DLNR foundation model is described in the paper: [High-frequency Stereo Matching Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf).\n",
    "In short, this algorithm works by rendering stereo pairs of images from multiple views of the Gaussian splat model, and using DLNR to compute depth maps from these stereo pairs. The depth maps and images are then integrated into a sparse **fvdb.Grid** in a narrow band around the surface using a weighted averaging scheme. The algorithm returns this grid along with signed distance values and colors (or other features) at each voxel.\n",
    "The mesh can then be extracted from the TSDF using the marching cubes algorithm implemented in **fvdb.marching_cubes.marching_cubes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8132b-87c2-406b-b1c6-43a32d364dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ply to mesh script\n",
    "# --ply_path: Input Gaussian splatting scene\n",
    "# --output_path: Where to save mesh\n",
    "# --truncation_margin: Margin for truncating the mesh, in world units\n",
    "!python extract_mesh_dlnr.py \\\n",
    "--ply_path ../../Data/safety_park_half/pretrained_splat.ply \\\n",
    "--output_path ../../Results/safety_park_isaac_files/safety_park_mesh.ply \\\n",
    "--truncation_margin .5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0649f68",
   "metadata": {},
   "source": [
    "#### Cropping and Converting\n",
    "\n",
    "As we saw in Supersplat, splats end up beyond the main focus of our scene. The training process tries to reconstruct the details in all the images, even details far in the horizon. This can lead to messy edges. You can remove the edges in Supersplat but we will crop both the mesh and PLY to focus on the center of our scene using the **create_isaac_sim_ready_files.py**. Isaac Sim also uses different world axes coordinates than COLMAP. It assumes Z+ is up rather than -Y, so we will rotate the scene accordingly. Additionally, to ensure our mesh will act as a collider in Isaac Sim we will make it water tight. This will fill in holes in the mesh and smooth it out. While we are at it we will convert the mesh from a PLY to an OBJ, a format compatible with Isaac Sim. Isaac Sim doesn't understand a Gaussian splatting scene in PLY format either, so we will have to convert ours into something Isaac Sim can render. We will convert it to a Universal Scene Description Zip (USDZ), this is a compressed version of the USD file type commonly used with Isaac Sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script to make Isaac Sim Ready Files\n",
    "# --input_splat: Location of Gaussian splatting scene\n",
    "# --input_mesh: Location of mesh (PLY)\n",
    "# --output_path: Where to save the OBJ and USDZ (no file extension)\n",
    "# --bbox: Box to crop to\n",
    "# --resolution: How detailed our mesh will be. Increase if you want more faces and vertices.\n",
    "!python scripts/create_isaac_ready_files.py \\\n",
    "--input-splat ../../Data/safety_park_half/pretrained_splat.ply \\\n",
    "--input-mesh ../../Data/safety_park_half/pretrained_mesh.ply \\\n",
    "--output-path ../../Results/safety_park_isaac_files/safety_park_cropped \\\n",
    "--bbox -100 -70 -20 110 90 20 \\\n",
    "--resolution 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d574cfe",
   "metadata": {},
   "source": [
    "## Part 2: Creating an Isaac Sim Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a007239",
   "metadata": {},
   "source": [
    "Isaac Sim 5.0 and above includes [NuRec (Neural Reconstruction) rendering](https://docs.isaacsim.omniverse.nvidia.com/5.0.0/assets/usd_assets_nurec.html), adding the functionality to render Gaussian splatting scenes among other neural volume methods. In this section we will go through the process of creating an environment from a Gaussian splatting scene and mesh. Then, using Isaac Lab, we will walk a Spot robot around the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0edac3",
   "metadata": {},
   "source": [
    "### Running Isaac Sim\n",
    "To run Isaac Sim you need to locate the isaacsim folder, navigate to the release subdirectory, and run the isaac-sim.sh file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../isaacsim/_build/linux-x86_64/release && ./isaac-sim.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83139cb0",
   "metadata": {},
   "source": [
    "After a few moments Isaac Sim will open and you should see the following window. \n",
    "\n",
    "<img src=\"./images/Isaac_sim_launch.png\" width=\"800\" style=\"display:block; margin:auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dd3b8",
   "metadata": {},
   "source": [
    "### Import the Assets\n",
    "Once Isaac Sim has launched you can import the Gaussian splatting scene and mesh made in [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting).\n",
    "1. In the content tab navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files**, inside this folder is the **safety_park_mesh_res_50000.obj**, drag the file into the stage window on the right.\n",
    "2. In the content window, still in **/home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files**, drag **safety_park_splats.usdz** into the stage window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc35737-d331-485f-9bbc-3bd3462dd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Isaac Sim file import demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/import_mesh_and_splats.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a59d1",
   "metadata": {},
   "source": [
    "Now the 3D Gaussian scene and the mesh should be overlapping in the viewer. The mesh will be very small at first, let's see how we can fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58b013",
   "metadata": {},
   "source": [
    "### Scene Setup\n",
    "Let's setup our scene so its ready for a robot.\n",
    "1. In the **Stage** tab click the **safety_park_mesh_res_50000** xform.\n",
    "2. In the **Property** tab, under **Transform**, change the **Scale:unitsResolve** to 1.0 for X, Y and Z.\n",
    "4. Back in the stage right click the **safety_park_mesh_res_50000** xform and select **Add > Physics > Colliders Preset**\n",
    "    * This makes it so other objects collide with our mesh instead passing right through it.\n",
    "5. Click the eye icon next to the **safety_park_mesh_res_50000** in the **Stage** window to hide the mesh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373d891-d389-4241-bba4-57dff7627319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the align splats and mesh in Isaac Sim demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/scene_setup.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db3117",
   "metadata": {},
   "source": [
    "Now the mesh and the 3D Gaussian scene are aligned. We also hid the mesh from view. It will still act as a collider but now we can use just high resolution splats for the visualization of Safety Park."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd9cfb",
   "metadata": {},
   "source": [
    "### Optional: Splat Color Editing\n",
    "Currently, lights don't interact with the Gaussian splatting scene, just the mesh. Since changing the strength of lights in the environment will have no effect on the look of the splats, we can artificially change the lighting by changing the emissive color values of the Gaussian splatting scene.\n",
    "1. In the **Stage** tab, select the **safety_park_splat > gauss > gauss > emissive_color_field** asset.\n",
    "2. In the **Property** tab, under **Raw USD Properties**\n",
    "    * Change Z in **emissive_color_field.omni:nurec:ccmB** to .7\n",
    "    * Change Y in **emissive_color_field.omni:nurec:ccmG** to .7\n",
    "    * Change X in **emissive_color_field.omni:nurec:ccmR** to .7\n",
    "\n",
    "These values represent the strength of emission for each of the 3 color channels. Be decreasing them all to 0.7 the splat looks less bright. Experiment with changing the values. Changing them non-uniformly will result in changes to the color of the scene, rather than just the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029d364-16d3-4b7f-89e7-7addc2204794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the ddjust splat colors in Isaac Sim demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/color.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b3298",
   "metadata": {},
   "source": [
    "### Save Scene\n",
    "To use the scene we created with Isaac Lab and a Spot robot we need to save it.\n",
    "1. Navigate to **File > Save As...**\n",
    "2. In the file browser that pops up navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Result/safety_park_isaac_files**\n",
    "3. Save the scene as **isaac_sim_scene.usd**.\n",
    "4. Exit out of Isaac Sim using the close button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d199a0-e5bb-4920-884e-9db7e1f8e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the save scene as USD demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/save_usd.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623ba96",
   "metadata": {},
   "source": [
    "### Isaac Lab and Robot Locomotion\n",
    "We can now use Isaac Lab and Isaac Sim to teleoperate a quadruped robot model (Boston Dynamics Spot) around our saved scene using a keyboard or a gaming controller. A locomotion policy was trained for this robot model using reinforcement learning in Isaac Lab, and we can inference this policy to translate velocity commands from the keyboard or controller into the joint-level actions required for the robot to walk. We need to launch Isaac Lab from outside our fVDB Python environment, so lets open a terminal.\n",
    "\n",
    "1. Open a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gnome-terminal --working-directory=/home/nvidia/Reconstructing_Outdoor_Environments/Code/IsaacLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da807f7d",
   "metadata": {},
   "source": [
    "2. To ensure Isaac Lab has access to as much GPU as possible, lets kill the Jupyter Notebook. Go to **File > Shut Down**. When promoted confirm you want to shut down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29df6a7",
   "metadata": {},
   "source": [
    "3. Isaac Lab needs to be ran outside of conda Python environments, including the fvdb environment and the base environment. Copy and paste the following command into the terminal so we are no longer working inside a conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681478d-203d-468b-be7d-245449a14fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate conda environments\n",
    "conda deactivate && conda deactivate && conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bec7bc",
   "metadata": {},
   "source": [
    "4. Now we can open our scene using Isaac Lab. It will launch a with Spot robot and its accompanying locomotion policy. Copy and paste the following command into the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827abd6-9af4-4c68-9427-1d0e782fab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Isaac Lab to launch Isaac Sim with with a Spot robot and location policy in Isaac Sim\n",
    "./isaaclab.sh -p /home/nvidia/Reconstructing_Outdoor_Environments/Code/robo_rl2/policy_inference_in_usd_safetypark.py --checkpoint /home/nvidia/Reconstructing_Outdoor_Environments/Code/robo_rl2/policy.pt --keyboard --terrain_usd /home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files/safety_park_isaac_scene.usd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e38c2",
   "metadata": {},
   "source": [
    "5. Use the arrow keys to move the Spot robot around. You can use the **X** and **Z** keys to control the yaw and turn the robot. Try running into objects to see how the collider is stopping the Spot.\n",
    "6. The camera should follow the Spot robot around the scene as you explore Safety Park. You can change the position of the camera in the **Isaac Lab** tab on the right. Try changing the **Camera Eye** and **Camera Target** to see how it changes your view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d16eb-db7c-43d8-91fb-9c56676e9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Isaac Lab demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/isaac_lab.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd019c8d",
   "metadata": {},
   "source": [
    "Congratulations! You've completed this course, *Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim*. You can now create a 3D Gaussian Splatting reconstuction of a scene and use that scene with NVIDIA Omniverse in Isaac Sim and Isaac Lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
